{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b3c524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/LangFlow/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from docx import Document\n",
    "import tiktoken\n",
    "import ollama\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "from langchain_text_splitters import (\n",
    "    TokenTextSplitter,\n",
    "    RecursiveCharacterTextSplitter\n",
    ")\n",
    "from langchain_google_genai import (\n",
    "    GoogleGenerativeAIEmbeddings,\n",
    "    ChatGoogleGenerativeAI\n",
    ")\n",
    "from langchain_community.document_loaders import UnstructuredWordDocumentLoader, Docx2txtLoader\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from tqdm.auto import tqdm as notebook_tqdm\n",
    "\n",
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "from langchain_experimental.text_splitter import SemanticChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5642e277",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# Load environment variables    \n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "pc_api_key = os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eae1600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docx_documents(file_path):\n",
    "    \"\"\"\n",
    "    Load documents from a .docx file.\n",
    "    \"\"\"\n",
    "    # Use the UnstructuredWordDocumentLoader to load the document\n",
    "    loader = UnstructuredWordDocumentLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # If no documents are found, try using Docx2TextLoader\n",
    "    if not documents:\n",
    "        docx_loader = Docx2txtLoader(file_path)\n",
    "        documents = docx_loader.load()\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d9b0b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaEmbeddingsWrapper:\n",
    "    def __init__(self, model_name=\"nomic-embed-text\"):\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"Interface expected by LangChain\"\"\"\n",
    "        # Process each text individually\n",
    "        all_embeddings = []\n",
    "        for text in texts:\n",
    "            response = ollama.embeddings(model=self.model_name, prompt=text)\n",
    "            all_embeddings.append(response['embedding'])\n",
    "        return all_embeddings\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        \"\"\"Also needed for some LangChain components\"\"\"\n",
    "        response = ollama.embeddings(model=self.model_name, prompt=text)\n",
    "        return response['embedding']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58dd143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents_into_chunks(pages):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks.\n",
    "    \"\"\"\n",
    "    # Initialize the text splitter with the wrapper\n",
    "    embeddings_wrapper = OllamaEmbeddingsWrapper(model_name=\"nomic-embed-text\")\n",
    "    semantic_chunker = SemanticChunker(\n",
    "        embeddings=embeddings_wrapper,\n",
    "        breakpoint_threshold_type=\"standard_deviation\",\n",
    "        number_of_chunks=100,\n",
    "        min_chunk_size=5,\n",
    "        breakpoint_threshold_amount=10\n",
    "    )\n",
    "    split_docs = semantic_chunker.split_documents(pages)\n",
    "\n",
    "# semantic_chunker = SemanticChunker(\n",
    "#         embeddings=ollama.embeddings(model=\"nomic-embed-text\"),\n",
    "#         breakpoint_threshold_type=\"standard_deviation\",\n",
    "#         number_of_chunks=100,\n",
    "#         breakpoint_threshold_amount=70\n",
    "#     )\n",
    "# docs = load_docx_documents(\"data/Sample_BRD_Policy_Management_System.docx\")\n",
    "# text = semantic_chunker.create_documents(docs[0].page_content)\n",
    "# len(text)  # Check the number of chunks created\n",
    "\n",
    "    token_splitter = TokenTextSplitter(\n",
    "        encoding_name=\"o200k_base\",\n",
    "        chunk_size=50,\n",
    "        chunk_overlap=0,\n",
    "    )\n",
    "    final_chunks = token_splitter.split_documents(split_docs)\n",
    "\n",
    "    return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afc9bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(chunks):\n",
    "    \"\"\"\n",
    "    Create a vector store from the chunks.\n",
    "    \"\"\"\n",
    "    embedder = OllamaEmbeddingsWrapper(model_name=\"nomic-embed-text\")\n",
    "    document_texts = [doc.page_content for doc in chunks]\n",
    "    document_embeddings = embedder.embed_documents(document_texts)  # Use embed_documents instead of embedding\n",
    "\n",
    "    dimension = len(document_embeddings[0]) if document_embeddings else 0  # In case document_embeddings is a list\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(np.array(document_embeddings).astype(np.float32))\n",
    "\n",
    "    return index, document_texts, embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9266d4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query, embedder, index, documents, k=3):\n",
    "    query_embedding = embedder.embed_query(query)  # Use embed_query instead of embedding\n",
    "    distances, indices = index.search(np.array([query_embedding]).astype(np.float32), k)\n",
    "    return [documents[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8432659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_ollama(query, context):\n",
    "    \"\"\"\n",
    "    Generate an answer using the Ollama model.\n",
    "    \"\"\"\n",
    "    formatted_context = \"\\n\".join(context)\n",
    "    prompt = f\"\"\"You are an expert trained on the following documents information:\n",
    "               {formatted_context}\n",
    "                \n",
    "                Answer the question: {query}\n",
    "\n",
    "                Answer correctly with maximum accuracy. Ensure that the answer is relevant to the question and the context provided.\n",
    "                Also mention in where the information was found in the documents.\n",
    "\"\"\"\n",
    "    response = ollama.generate(\n",
    "        model=\"llama3.2:1b\",\n",
    "        prompt=prompt,\n",
    "        options={\n",
    "            \"temperature\": 0.4\n",
    "        }\n",
    "    )\n",
    "    return response['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9295dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(query):\n",
    "    # Load and process DOCX file\n",
    "    file_path = \"data/Sample_BRD_Policy_Management_System.docx\"\n",
    "    pages = load_docx_documents(file_path)\n",
    "    split_docs = split_documents_into_chunks(pages)\n",
    "    \n",
    "    # Create vector store\n",
    "    index, document_texts, embedder = create_vector_store(split_docs)\n",
    "    \n",
    "    # Retrieve context\n",
    "    context = retrieve_context(query, embedder, index, document_texts)\n",
    "    \n",
    "    # Generate answer\n",
    "    answer = generate_answer_ollama(query, context)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cd035b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Markdown Formatted Answer -----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "# Response to Query: \"Find rules related to increasing Sum Insured for Activ Health policies.\"\n",
       "\n",
       "Based on the provided Upsell Criteria Rules, I've identified the following rules related to increasing Sum Insured for Activ Health policies:\n",
       "\n",
       "1. **Age Limit**: The maximum age limit for activating an Activ Health policy is 45 years (Rule: Age Limit â‰¤50 years). This rule indicates that the policy can be activated at any age up to 50 years.\n",
       "\n",
       "2. **Max Sum Insured**: There are no specific rules related to increasing the max sum insured for Activ Health policies in this document. However, it's essential to note that the maximum sum insured is typically determined by the insurance company and may vary depending on individual circumstances.\n",
       "\n",
       "3. **Activ One**: The information provided does not mention any rules or criteria related to activating an Activ One policy. It seems that Activ One is a separate type of policy or product offered by the insurance company, and this document only provides guidelines for Activ Health policies.\n",
       "\n",
       "4. **Sum Insured Increase Criteria**: There are no specific rules or criteria mentioned in this document that would allow the sum insured to be increased based on certain conditions. The focus seems to be on determining the maximum age limit and age-related restrictions rather than adjusting the sum insured amount.\n",
       "\n",
       "The information was found in:\n",
       "\n",
       "* Upsell Criteria Rules (not explicitly stated, but implied as a separate document or section within the main policy documents)\n",
       "* Policy terms and conditions for Activ Health policies\n",
       "* Insurance company's website, policy documentation, or customer support resources.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import markdown\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_markdown_result():\n",
    "    markdown_result = f\"\"\"\n",
    "# Response to Query: \"{query}\"\n",
    "\n",
    "{result}\n",
    "\"\"\"\n",
    "    display(Markdown(markdown_result))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"Find rules related to increasing Sum Insured for Activ Health policies.\"\n",
    "    \n",
    "    result = main(query)\n",
    "    #print(\"Raw Answer:\", result)\n",
    "    print(\"\\n----- Markdown Formatted Answer -----\\n\")\n",
    "    display_markdown_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dd4961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719353a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
